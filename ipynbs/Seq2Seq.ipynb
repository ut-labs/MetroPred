{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import mxnet as mx\n",
    "from mxnet import autograd, gluon, nd\n",
    "from mxnet.gluon import nn, rnn, Block\n",
    "from mxnet.contrib import text\n",
    "\n",
    "import collections\n",
    "import datetime\n",
    "import pickle\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "epochs = 500\n",
    "epoch_period = 1\n",
    "\n",
    "learning_rate = 0.01\n",
    "batch_size = 8\n",
    "\n",
    "max_seq_len = 144\n",
    "max_output_len = 144\n",
    "\n",
    "encoder_num_layers = 2\n",
    "decoder_num_layers = 4\n",
    "\n",
    "encoder_drop_prob = 0.1\n",
    "decoder_drop_prob = 0.1\n",
    "\n",
    "encoder_hidden_dim = 256\n",
    "decoder_hidden_dim = 512\n",
    "alignment_dim = 512\n",
    "\n",
    "ctx = mx.gpu(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1863, 144, 2) (1863, 144, 2)\n"
     ]
    }
   ],
   "source": [
    "X = []\n",
    "Y = []\n",
    "for i in range(1,24):\n",
    "    d1 = np.load('train/%d.npy'%(i))\n",
    "    d2 = np.load('train/%d.npy'%(i+1))\n",
    "    for s_id in range(81):\n",
    "        X.append(d1[:,s_id,:])\n",
    "        Y.append(d2[:,s_id,:])\n",
    "X = np.array(X)\n",
    "Y = np.array(Y)\n",
    "print(X.shape, Y.shape)\n",
    "dataset = gluon.data.ArrayDataset(nd.array(X,ctx=ctx), nd.array(Y,ctx=ctx))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "-------------------------------------------------"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Encoder(Block):\n",
    "    def __init__(self, input_dim, hidden_dim, num_layers, drop_prob, **kwargs):\n",
    "        super(Encoder, self).__init__(**kwargs)\n",
    "        with self.name_scope():\n",
    "            self.fc = nn.Dense(input_dim, hidden_dim, flatten=True)\n",
    "            self.rnn = rnn.GRU(hidden_dim, num_layers, dropout=drop_prob, input_size=hidden_dim)\n",
    "    \n",
    "    def forward(self, inputs, state):\n",
    "        # rnn input shape : seq_len * bs * 2\n",
    "        inputs = self.fc(inputs).swapaxes(0,1)\n",
    "        output, state = self.rnn(inputs, state)\n",
    "        return output, state\n",
    "\n",
    "    def begin_state(self, *args, **kwargs):\n",
    "        return self.rnn.begin_state(*args, **kwargs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Decoder(Block):\n",
    "    \"\"\"含注意力机制的解码器\"\"\"\n",
    "    def __init__(self, hidden_dim, output_dim, num_layers, max_seq_len,\n",
    "                 drop_prob, alignment_dim, encoder_hidden_dim, **kwargs):\n",
    "        super(Decoder, self).__init__(**kwargs)\n",
    "        self.max_seq_len = max_seq_len\n",
    "        self.encoder_hidden_dim = encoder_hidden_dim\n",
    "        self.hidden_size = hidden_dim\n",
    "        self.num_layers = num_layers\n",
    "        with self.name_scope():\n",
    "            #self.embedding = nn.Embedding(output_dim, hidden_dim)\n",
    "            self.dropout = nn.Dropout(drop_prob)\n",
    "            # 注意力机制。\n",
    "            self.attention = nn.Sequential()\n",
    "            with self.attention.name_scope():\n",
    "                self.attention.add(nn.Dense(\n",
    "                    alignment_dim, in_units=hidden_dim + encoder_hidden_dim,\n",
    "                    activation=\"tanh\", flatten=False))\n",
    "                self.attention.add(nn.Dense(1, in_units=alignment_dim,\n",
    "                                            flatten=False))\n",
    "\n",
    "            self.rnn = rnn.GRU(hidden_dim, num_layers, dropout=drop_prob,\n",
    "                               input_size=hidden_dim)\n",
    "            self.out = nn.Dense(output_dim, in_units=hidden_dim, flatten=False)\n",
    "            self.rnn_concat_input = nn.Dense(\n",
    "                hidden_dim, in_units=hidden_dim + encoder_hidden_dim,\n",
    "                flatten=False)\n",
    "\n",
    "    def forward(self, cur_input, state, encoder_outputs):\n",
    "        # 当RNN为多层时，取最靠近输出层的单层隐含状态。\n",
    "        single_layer_state = [state[0][-1].expand_dims(0)]\n",
    "        encoder_outputs = encoder_outputs.reshape((self.max_seq_len, -1,\n",
    "                                                   self.encoder_hidden_dim))\n",
    "        # single_layer_state尺寸: [(1, batch_size, decoder_hidden_dim)]\n",
    "        # hidden_broadcast尺寸: (max_seq_len, batch_size, decoder_hidden_dim)\n",
    "        hidden_broadcast = nd.broadcast_axis(single_layer_state[0], axis=0,\n",
    "                                             size=self.max_seq_len)\n",
    "\n",
    "        # encoder_outputs_and_hiddens尺寸:\n",
    "        # (max_seq_len, batch_size, encoder_hidden_dim + decoder_hidden_dim)\n",
    "        encoder_outputs_and_hiddens = nd.concat(encoder_outputs,\n",
    "                                                hidden_broadcast, dim=2)\n",
    "\n",
    "        # energy尺寸: (max_seq_len, batch_size, 1)\n",
    "        energy = self.attention(encoder_outputs_and_hiddens)\n",
    "\n",
    "        # batch_attention尺寸: (batch_size, 1, max_seq_len)\n",
    "        batch_attention = nd.softmax(energy, axis=0).transpose(\n",
    "            (1, 2, 0))\n",
    "\n",
    "        # batch_encoder_outputs尺寸: (batch_size, max_seq_len, encoder_hidden_dim)\n",
    "        batch_encoder_outputs = encoder_outputs.swapaxes(0, 1)\n",
    "\n",
    "        # decoder_context尺寸: (batch_size, 1, encoder_hidden_dim)\n",
    "        decoder_context = nd.batch_dot(batch_attention, batch_encoder_outputs)\n",
    "\n",
    "        # cur_input尺寸: (batch_size,)\n",
    "        # input_and_context尺寸: (batch_size, 1, encoder_hidden_dim + decoder_hidden_dim)\n",
    "        input_and_context = nd.concat(nd.expand_dims(self.embedding(cur_input), axis=1),\n",
    "                                      decoder_context, dim=2)\n",
    "        # concat_input尺寸: (1, batch_size, decoder_hidden_dim)\n",
    "        concat_input = self.rnn_concat_input(input_and_context).reshape((1, -1, 0))\n",
    "        concat_input = self.dropout(concat_input)\n",
    "\n",
    "        # 当RNN为多层时，用单层隐含状态初始化各个层的隐含状态。\n",
    "        state = [nd.broadcast_axis(single_layer_state[0], axis=0,\n",
    "                                   size=self.num_layers)]\n",
    "\n",
    "        output, state = self.rnn(concat_input, state)\n",
    "        output = self.dropout(output)\n",
    "        output = self.out(output).reshape((-3, -1))\n",
    "        # output尺寸: (batch_size, output_size)\n",
    "        return output, state\n",
    "\n",
    "    def begin_state(self, *args, **kwargs):\n",
    "        return self.rnn.begin_state(*args, **kwargs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DecoderInitState(Block):\n",
    "    \"\"\"解码器隐含状态的初始化\"\"\"\n",
    "    def __init__(self, encoder_hidden_dim, decoder_hidden_dim, **kwargs):\n",
    "        super(DecoderInitState, self).__init__(**kwargs)\n",
    "        with self.name_scope():\n",
    "            self.dense = nn.Dense(decoder_hidden_dim,\n",
    "                                  in_units=encoder_hidden_dim,\n",
    "                                  activation=\"tanh\", flatten=False)\n",
    "\n",
    "    def forward(self, encoder_state):\n",
    "        return [self.dense(encoder_state)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "unsupported operand type(s) for +: 'int' and 'str'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-22-eeb6a09b0710>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m encoder = Encoder(144, encoder_hidden_dim, encoder_num_layers,\n\u001b[0;32m----> 2\u001b[0;31m                   encoder_drop_prob)\n\u001b[0m\u001b[1;32m      3\u001b[0m decoder = Decoder(decoder_hidden_dim, 2,\n\u001b[1;32m      4\u001b[0m                   \u001b[0mdecoder_num_layers\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmax_seq_len\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdecoder_drop_prob\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m                   alignment_dim, encoder_hidden_dim)\n",
      "\u001b[0;32m<ipython-input-19-0c866bd5ee5e>\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, input_dim, hidden_dim, num_layers, drop_prob, **kwargs)\u001b[0m\n\u001b[1;32m      3\u001b[0m         \u001b[0msuper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mEncoder\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__init__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m         \u001b[0;32mwith\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mname_scope\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 5\u001b[0;31m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfc\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mDense\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput_dim\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhidden_dim\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mflatten\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      6\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrnn\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mrnn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mGRU\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mhidden_dim\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnum_layers\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdropout\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdrop_prob\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mhidden_dim\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.7/site-packages/mxnet/gluon/nn/basic_layers.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, units, activation, use_bias, flatten, dtype, weight_initializer, bias_initializer, in_units, **kwargs)\u001b[0m\n\u001b[1;32m    213\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbias\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    214\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mactivation\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 215\u001b[0;31m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mact\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mActivation\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mactivation\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mprefix\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mactivation\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0;34m'_'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    216\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    217\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mact\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mTypeError\u001b[0m: unsupported operand type(s) for +: 'int' and 'str'"
     ]
    }
   ],
   "source": [
    "encoder = Encoder(144, encoder_hidden_dim, encoder_num_layers,\n",
    "                  encoder_drop_prob)\n",
    "decoder = Decoder(decoder_hidden_dim, 2,\n",
    "                  decoder_num_layers, max_seq_len, decoder_drop_prob,\n",
    "                  alignment_dim, encoder_hidden_dim)\n",
    "decoder_init_state = DecoderInitState(encoder_hidden_dim, decoder_hidden_dim)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "encoder.collect_params().initialize(mx.init.Xavier(), ctx=ctx)\n",
    "decoder.collect_params().initialize(mx.init.Xavier(), ctx=ctx)\n",
    "decoder_init_state.collect_params().initialize(mx.init.Xavier(), ctx=ctx)\n",
    "\n",
    "softmax_cross_entropy = gluon.loss.L1Loss()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(8, 144, 2)"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "ename": "MXNetError",
     "evalue": "Shape inconsistent, Provided = [789504], inferred shape=(594432,)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mMXNetError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-10-ae85eaa9cfed>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     21\u001b[0m             encoder_state = encoder.begin_state(\n\u001b[1;32m     22\u001b[0m                 func=mx.nd.zeros, batch_size=real_batch_size, ctx=ctx)\n\u001b[0;32m---> 23\u001b[0;31m             \u001b[0mencoder_outputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mencoder_state\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mencoder\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mencoder_state\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     24\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     25\u001b[0m             \u001b[0;31m# encoder_outputs尺寸: (max_seq_len, encoder_hidden_dim)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.7/site-packages/mxnet/gluon/block.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args)\u001b[0m\n\u001b[1;32m    538\u001b[0m             \u001b[0mhook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    539\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 540\u001b[0;31m         \u001b[0mout\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    541\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    542\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mhook\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_forward_hooks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-5-5d651f2aac44>\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, inputs, state)\u001b[0m\n\u001b[1;32m      8\u001b[0m         \u001b[0;31m# rnn input shape : seq_len * bs * 2\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      9\u001b[0m         \u001b[0minputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mswapaxes\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 10\u001b[0;31m         \u001b[0moutput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstate\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrnn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstate\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     11\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0moutput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstate\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     12\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.7/site-packages/mxnet/gluon/block.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args)\u001b[0m\n\u001b[1;32m    538\u001b[0m             \u001b[0mhook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    539\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 540\u001b[0;31m         \u001b[0mout\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    541\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    542\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mhook\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_forward_hooks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.7/site-packages/mxnet/gluon/block.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, x, *args)\u001b[0m\n\u001b[1;32m    915\u001b[0m                     \u001b[0mparams\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m{\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mj\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mctx\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mj\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_reg_params\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mitems\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    916\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 917\u001b[0;31m                 \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mhybrid_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mndarray\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mparams\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    918\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    919\u001b[0m         \u001b[0;32massert\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mSymbol\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;31m \u001b[0m\u001b[0;31m\\\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.7/site-packages/mxnet/gluon/rnn/rnn_layer.py\u001b[0m in \u001b[0;36mhybrid_forward\u001b[0;34m(self, F, inputs, states, **kwargs)\u001b[0m\n\u001b[1;32m    232\u001b[0m                         \"Invalid recurrent state shape. Expecting %s, got %s.\"%(\n\u001b[1;32m    233\u001b[0m                             str(info['shape']), str(state.shape)))\n\u001b[0;32m--> 234\u001b[0;31m         \u001b[0mout\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_forward_kernel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mF\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstates\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    235\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    236\u001b[0m         \u001b[0;31m# out is (output, state)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.7/site-packages/mxnet/gluon/rnn/rnn_layer.py\u001b[0m in \u001b[0;36m_forward_kernel\u001b[0;34m(self, F, inputs, states, **kwargs)\u001b[0m\n\u001b[1;32m    263\u001b[0m                     \u001b[0mlstm_state_clip_min\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_lstm_state_clip_min\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    264\u001b[0m                     \u001b[0mlstm_state_clip_max\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_lstm_state_clip_max\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 265\u001b[0;31m                     lstm_state_clip_nan=self._lstm_state_clip_nan)\n\u001b[0m\u001b[1;32m    266\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    267\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_mode\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m'lstm'\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.7/site-packages/mxnet/ndarray/register.py\u001b[0m in \u001b[0;36mRNN\u001b[0;34m(data, parameters, state, state_cell, state_size, num_layers, bidirectional, mode, p, state_outputs, projection_size, lstm_state_clip_min, lstm_state_clip_max, lstm_state_clip_nan, out, name, **kwargs)\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.7/site-packages/mxnet/_ctypes/ndarray.py\u001b[0m in \u001b[0;36m_imperative_invoke\u001b[0;34m(handle, ndargs, keys, vals, out)\u001b[0m\n\u001b[1;32m     90\u001b[0m         \u001b[0mc_str_array\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkeys\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     91\u001b[0m         \u001b[0mc_str_array\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mstr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0ms\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0ms\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mvals\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 92\u001b[0;31m         ctypes.byref(out_stypes)))\n\u001b[0m\u001b[1;32m     93\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     94\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0moriginal_output\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.7/site-packages/mxnet/base.py\u001b[0m in \u001b[0;36mcheck_call\u001b[0;34m(ret)\u001b[0m\n\u001b[1;32m    250\u001b[0m     \"\"\"\n\u001b[1;32m    251\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mret\u001b[0m \u001b[0;34m!=\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 252\u001b[0;31m         \u001b[0;32mraise\u001b[0m \u001b[0mMXNetError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpy_str\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0m_LIB\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mMXGetLastError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    253\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    254\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mMXNetError\u001b[0m: Shape inconsistent, Provided = [789504], inferred shape=(594432,)"
     ]
    }
   ],
   "source": [
    "encoder_optimizer = gluon.Trainer(encoder.collect_params(), 'adam',\n",
    "                                  {'learning_rate': learning_rate})\n",
    "decoder_optimizer = gluon.Trainer(decoder.collect_params(), 'adam',\n",
    "                                  {'learning_rate': learning_rate})\n",
    "decoder_init_state_optimizer = gluon.Trainer(\n",
    "    decoder_init_state.collect_params(), 'adam',\n",
    "    {'learning_rate': learning_rate})\n",
    "\n",
    "prev_time = datetime.datetime.now()\n",
    "data_iter = gluon.data.DataLoader(dataset, batch_size, shuffle=True)\n",
    "\n",
    "total_loss = 0.0\n",
    "iter_times = 0\n",
    "\n",
    "for epoch in range(1, epochs + 1):\n",
    "    for x, y in data_iter:\n",
    "        real_batch_size = x.shape[0]\n",
    "        with autograd.record():\n",
    "            loss = nd.array([0], ctx=ctx)\n",
    "            valid_length = nd.array([0], ctx=ctx)\n",
    "            encoder_state = encoder.begin_state(\n",
    "                func=mx.nd.zeros, batch_size=real_batch_size, ctx=ctx)\n",
    "            encoder_outputs, encoder_state = encoder(x, encoder_state)\n",
    "\n",
    "            # encoder_outputs尺寸: (max_seq_len, encoder_hidden_dim)\n",
    "            encoder_outputs = encoder_outputs.flatten()\n",
    "            # 解码器的第一个输入为BOS字符。\n",
    "            decoder_input = nd.array([0] * real_batch_size,\n",
    "                                     ctx=ctx)\n",
    "            mask = nd.ones(shape=(real_batch_size,), ctx=ctx)\n",
    "            decoder_state = decoder_init_state(encoder_state[0])\n",
    "            for i in range(max_seq_len):\n",
    "                decoder_output, decoder_state = decoder(\n",
    "                    decoder_input, decoder_state, encoder_outputs)\n",
    "                # 解码器使用当前时刻的预测结果作为下一时刻的输入。\n",
    "                decoder_input = decoder_output\n",
    "                loss = loss + softmax_cross_entropy(decoder_output, y[:, i])\n",
    "            loss = loss / 144\n",
    "            loss = nd.mean(loss)\n",
    "        loss.backward()\n",
    "\n",
    "        encoder_optimizer.step(1)\n",
    "        decoder_optimizer.step(1)\n",
    "        decoder_init_state_optimizer.step(1)\n",
    "\n",
    "        total_loss += loss.asscalar() / max_seq_len\n",
    "        iter_times += 1\n",
    "        \n",
    "    if epoch % epoch_period == 0 or epoch == 1:\n",
    "        cur_time = datetime.datetime.now()\n",
    "        h, remainder = divmod((cur_time - prev_time).seconds, 3600)\n",
    "        m, s = divmod(remainder, 60)\n",
    "        time_str = 'Time %02d:%02d:%02d' % (h, m, s)\n",
    "        if epoch == 1:\n",
    "            print_loss_avg = total_loss / len(data_iter)\n",
    "        else:\n",
    "            print_loss_avg = total_loss / epoch_period / len(data_iter)\n",
    "        loss_str = 'Epoch %d, Loss %f, ' % (epoch, print_loss_avg)\n",
    "        print(loss_str + time_str)\n",
    "        if epoch != 1:\n",
    "            total_loss = 0.0\n",
    "        prev_time = cur_time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test(x, y):\n",
    "    nd_x = x\n",
    "    x = [int(_) for _ in x.asnumpy().tolist()]\n",
    "    y = [int(_) for _ in y.asnumpy().tolist()]\n",
    "    print('Input: ',' '.join(input_vocab.to_tokens(x)).split('<eos>')[0])\n",
    "    #print('Expect: ',' '.join(output_vocab.to_tokens(y)))\n",
    "    \n",
    "    encoder_state = encoder.begin_state(func=mx.nd.zeros, batch_size=1, ctx=ctx)\n",
    "    encoder_outputs, encoder_state = encoder(nd_x.expand_dims(0), encoder_state)\n",
    "    encoder_outputs = encoder_outputs.flatten()\n",
    "    \n",
    "    decoder_input = nd.array([output_vocab.token_to_idx[BOS]], ctx=ctx)\n",
    "    decoder_state = decoder_init_state(encoder_state[0])\n",
    "    output_tokens = []\n",
    "    \n",
    "    for _ in range(max_output_len):\n",
    "        decoder_output, decoder_state = decoder(\n",
    "            decoder_input, decoder_state, encoder_outputs)\n",
    "        pred_i = int(decoder_output.argmax(axis=1).asnumpy()[0])\n",
    "        if pred_i == output_vocab.token_to_idx[EOS]:\n",
    "            break\n",
    "        else:\n",
    "            output_tokens.append(output_vocab.idx_to_token[pred_i])\n",
    "        decoder_input = nd.array([pred_i], ctx=ctx)\n",
    "    print('Output:', ' '.join(output_tokens))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test(x):\n",
    "    print('Input: ', x)\n",
    "    input_tokens = x.split(' ') + [EOS]\n",
    "    while len(input_tokens) < max_seq_len:\n",
    "        input_tokens.append(PAD)\n",
    "    inputs = nd.array(input_vocab.to_indices(input_tokens), ctx=ctx)\n",
    "    encoder_state = encoder.begin_state(func=mx.nd.zeros, batch_size=1, ctx=ctx)\n",
    "    encoder_outputs, encoder_state = encoder(inputs.expand_dims(0), encoder_state)\n",
    "    encoder_outputs = encoder_outputs.flatten()\n",
    "    \n",
    "    decoder_state = decoder_init_state(encoder_state[0])\n",
    "    decoder_input = nd.array([output_vocab.token_to_idx[BOS]], ctx=ctx)\n",
    "    output_tokens = []\n",
    "    \n",
    "    for _ in range(max_output_len):\n",
    "        decoder_output, decoder_state = decoder(\n",
    "            decoder_input, decoder_state, encoder_outputs)\n",
    "        pred_i = int(decoder_output.argmax(axis=1).asnumpy()[0])\n",
    "        if pred_i == output_vocab.token_to_idx[EOS]:\n",
    "            break\n",
    "        else:\n",
    "            output_tokens.append(output_vocab.idx_to_token[pred_i])\n",
    "        decoder_input = nd.array([pred_i], ctx=ctx)\n",
    "    print('Output: ', ' '.join(output_tokens))\n",
    "    return ' '.join(output_tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('test_data.txt','r') as f:\n",
    "    test_data = f.readlines()\n",
    "test_data = [data[:-1] for data in test_data]\n",
    "ans = []\n",
    "for data in test_data:\n",
    "    ans.append(test(data))\n",
    "with open('ans-none.pickle','wb') as f:\n",
    "    pickle.dump(ans,f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(0,2890):\n",
    "    test(X[i],Y[i])\n",
    "    print('\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "eval_post_resp = []\n",
    "for i in range(30):\n",
    "    eval_post_resp.append([' '.join(input_seqs[i]), ' '.join(output_seqs[i])])\n",
    "beam_size = 1\n",
    "beam_search_test(encoder, decoder, decoder_init_state, eval_post_resp, ctx, max_seq_len)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def beam_search_test(encoder, decoder, decoder_init_state, eval_post_resp, ctx, max_seq_len):\n",
    "    for p_r in eval_post_resp:\n",
    "        print('Input: ', p_r[0])\n",
    "        input_tokens = p_r[0].split(' ') + [EOS]\n",
    "        while len(input_tokens) < max_seq_len:\n",
    "            input_tokens.append(PAD)\n",
    "        inputs = nd.array(input_vocab.to_indices(input_tokens), ctx = ctx)\n",
    "        encoder_state = encoder.begin_state(func=mx.nd.zeros, batch_size=1, ctx=ctx)\n",
    "        encoder_outputs, encoder_state = encoder(inputs.expand_dims(0), encoder_state)\n",
    "        encoder_outputs = encoder_outputs.flatten()\n",
    "        \n",
    "        #decoder_input = nd.array([output_vocab.token_to_idx[BOS]], ctx=ctx)\n",
    "        decoder_state = decoder_init_state(encoder_state[0])\n",
    "        \n",
    "        candidates = [[BOS] for _ in range(beam_size)]\n",
    "        probs = [0.0 for _ in range(beam_size)]\n",
    "        \n",
    "        for _ in range(max_output_len):\n",
    "            tmp = []\n",
    "            tmp_cand = []\n",
    "            for k in range(beam_size):\n",
    "                decoder_input = nd.array([output_vocab.token_to_idx[candidates[k][-1]]], ctx=ctx)\n",
    "                decoder_output, decoder_state = decoder(decoder_input, decoder_state, encoder_outputs)\n",
    "                \n",
    "                pred_score, pred_index = decoder_output.topk(ret_typ='both',k=beam_size)\n",
    "                #pred_i = int(pred_index[0].asnumpy()[0])\n",
    "                for j in range(beam_size):\n",
    "                    tmp.append(probs[k] + pred_score[0].asnumpy()[j])\n",
    "                    tmp_cand.append(candidates[k] + [output_vocab.idx_to_token[int(pred_index[0].asnumpy()[j])]])\n",
    "            top_k_idx = np.argsort(tmp)[-beam_size:]\n",
    "            for k in range(beam_size):\n",
    "                candidates[k] = tmp_cand[top_k_idx[k]]\n",
    "                probs[k] = tmp[top_k_idx[k]]\n",
    "                #print(' '.join(candidates[k]))\n",
    "        #print(probs)\n",
    "        top_idx = np.argsort(probs).tolist()\n",
    "        #print(top_idx)\n",
    "        print('Output: ')\n",
    "        for idx in top_idx:\n",
    "            print(' '.join(candidates[idx]), probs[idx])\n",
    "        print('Expect:', p_r[1], '\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "encoder.collect_params().save('encoder-bless-L118.params')\n",
    "decoder.collect_params().save('decoder-bless-L118.params')\n",
    "decoder_init_state.collect_params().save('decoder-init-state-bless-L118.params')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
